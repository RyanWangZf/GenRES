{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/textgen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "cdr_rel2id = {'1:NR:2': 0, '1:CID:2': 1}\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1', cache_dir='/data/pj20/.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2triples = {}\n",
    "\n",
    "def chunks(l, n):\n",
    "    res = []\n",
    "    for i in range(0, len(l), n):\n",
    "        assert len(l[i:i + n]) == n\n",
    "        res += [l[i:i + n]]\n",
    "    return res\n",
    "\n",
    "def read_cdr(file_in, tokenizer):\n",
    "    pmids = set()\n",
    "    with open(file_in, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "        for i_l, line in enumerate(tqdm(lines)):\n",
    "            line = line.rstrip().split('\\t')\n",
    "            pmid = line[0]\n",
    "\n",
    "            if pmid not in pmids:\n",
    "                pmids.add(pmid)\n",
    "                text = line[1]\n",
    "                prs = chunks(line[2:], 17)\n",
    "\n",
    "                ent2idx = {}\n",
    "                train_triples = {}\n",
    "\n",
    "                entity_pos = set()\n",
    "                for p in prs:\n",
    "                    es = list(map(int, p[8].split(':')))\n",
    "                    ed = list(map(int, p[9].split(':')))\n",
    "                    tpy = p[7]\n",
    "                    for start, end in zip(es, ed):\n",
    "                        entity_pos.add((start, end, tpy))\n",
    "\n",
    "                    es = list(map(int, p[14].split(':')))\n",
    "                    ed = list(map(int, p[15].split(':')))\n",
    "                    tpy = p[13]\n",
    "                    for start, end in zip(es, ed):\n",
    "                        entity_pos.add((start, end, tpy))\n",
    "\n",
    "                sents = [t.split(' ') for t in text.split('|')]\n",
    "                new_sents = []\n",
    "                sent_map = {}\n",
    "                i_t = 0\n",
    "                for sent in sents:\n",
    "                    for token in sent:\n",
    "                        tokens_wordpiece = tokenizer.tokenize(token)\n",
    "                        for start, end, tpy in list(entity_pos):\n",
    "                            if i_t == start:\n",
    "                                tokens_wordpiece = [\"*\"] + tokens_wordpiece\n",
    "                            if i_t + 1 == end:\n",
    "                                tokens_wordpiece = tokens_wordpiece + [\"*\"]\n",
    "                        sent_map[i_t] = len(new_sents)\n",
    "                        new_sents.extend(tokens_wordpiece)\n",
    "                        i_t += 1\n",
    "                    sent_map[i_t] = len(new_sents)\n",
    "                sents = new_sents\n",
    "\n",
    "                entity_pos = []\n",
    "\n",
    "                for p in prs:\n",
    "                    if p[0] == \"not_include\":\n",
    "                        continue\n",
    "                    if p[1] == \"L2R\":\n",
    "                        h_id, t_id = p[5], p[11]\n",
    "                        h_start, t_start = p[8], p[14]\n",
    "                        h_end, t_end = p[9], p[15]\n",
    "                    else:\n",
    "                        t_id, h_id = p[5], p[11]\n",
    "                        t_start, h_start = p[8], p[14]\n",
    "                        t_end, h_end = p[9], p[15]\n",
    "                    h_start = map(int, h_start.split(':'))\n",
    "                    h_end = map(int, h_end.split(':'))\n",
    "                    t_start = map(int, t_start.split(':'))\n",
    "                    t_end = map(int, t_end.split(':'))\n",
    "                    h_start = [sent_map[idx] for idx in h_start]\n",
    "                    h_end = [sent_map[idx] for idx in h_end]\n",
    "                    t_start = [sent_map[idx] for idx in t_start]\n",
    "                    t_end = [sent_map[idx] for idx in t_end]\n",
    "                    if h_id not in ent2idx:\n",
    "                        ent2idx[h_id] = len(ent2idx)\n",
    "                        entity_pos.append(list(zip(h_start, h_end)))\n",
    "                    if t_id not in ent2idx:\n",
    "                        ent2idx[t_id] = len(ent2idx)\n",
    "                        entity_pos.append(list(zip(t_start, t_end)))\n",
    "                    h_id, t_id = ent2idx[h_id], ent2idx[t_id]\n",
    "\n",
    "                    r = cdr_rel2id[p[0]]\n",
    "                    if (h_id, t_id) not in train_triples:\n",
    "                        train_triples[(h_id, t_id)] = [{'relation': r}]\n",
    "                    else:\n",
    "                        train_triples[(h_id, t_id)].append({'relation': r})\n",
    "\n",
    "                relations, hts = [], []\n",
    "                for h, t in train_triples.keys():\n",
    "                    relation = [0] * len(cdr_rel2id)\n",
    "                    for mention in train_triples[h, t]:\n",
    "                        relation[mention[\"relation\"]] = 1\n",
    "                    relations.append(relation)\n",
    "                    hts.append([h, t])\n",
    "\n",
    "                text2triples[pmid] = {'text': sents, 'relations': relations, 'hts': hts}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_features \u001b[39m=\u001b[39m read_cdr(\u001b[39m'\u001b[39;49m\u001b[39m/home/pj20/GREScore/datasets/cdr/test_filter.data\u001b[39;49m\u001b[39m'\u001b[39;49m, tokenizer)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mread_cdr\u001b[0;34m(file_in, tokenizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_in, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m infile:\n\u001b[1;32m     13\u001b[0m     lines \u001b[39m=\u001b[39m infile\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mfor\u001b[39;00m i_l, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(lines)):\n\u001b[1;32m     15\u001b[0m         line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m         pmid \u001b[39m=\u001b[39m line[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "test_features = read_cdr('/home/pj20/GREScore/datasets/cdr/test_filter.data', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/pj20/GREScore/datasets/cdr/test_filter.data', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8701013\\tFamotidine - associated delirium .|A series of six cases .|Famotidine is a histamine H2 - receptor antagonist used in inpatient settings for prevention of stress ulcers and is showing increasing popularity because of its low cost .|Although all of the currently available H2 - receptor antagonists have shown the propensity to cause delirium , only two previously reported cases have been associated with famotidine .|The authors report on six cases of famotidine - associated delirium in hospitalized patients who cleared completely upon removal of famotidine .|The pharmacokinetics of famotidine are reviewed , with no change in its metabolism in the elderly population seen .|The implications of using famotidine in elderly persons are discussed .\\t1:CID:2\\tL2R\\tNON-CROSS\\t0-1\\t3-4\\tD015738\\tFamotidine|Famotidine|famotidine|famotidine|famotidine|famotidine|famotidine\\tChemical\\t0:11:66:75:88:93:113\\t1:12:67:76:89:94:114\\t0:2:3:4:4:5:6\\tD003693\\tdelirium|delirium|delirium\\tDisease\\t3:55:78\\t4:56:79\\t0:3:4\\t1:NR:2\\tL2R\\tNON-CROSS\\t11-12\\t27-28\\tD015738\\tFamotidine|Famotidine|famotidine|famotidine|famotidine|famotidine|famotidine\\tChemical\\t0:11:66:75:88:93:113\\t1:12:67:76:89:94:114\\t0:2:3:4:4:5:6\\tD014456\\tulcers\\tDisease\\t27\\t28\\t2\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('textgen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72e4df05eb8a5f0bdb80d33a9292878c0e2ee2d3f5bd7213be092f7893c82c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
