{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./docred_rand_200.json\", 'r') as file:\n",
    "    docred = json.load(file)\n",
    "    \n",
    "with open(\"./wiki20m_rand_500.json\", 'r') as file:\n",
    "    wiki20m = json.load(file)\n",
    "    \n",
    "with open(\"./wiki80_rand_800.json\", 'r') as file:\n",
    "    wiki80 = json.load(file)\n",
    "    \n",
    "original_rel_set = set()\n",
    "\n",
    "for dataset in [docred, wiki20m, wiki80]:\n",
    "    for text in dataset.keys():\n",
    "        triples = dataset[text]\n",
    "        for triple in triples:\n",
    "            relation = triple[2]\n",
    "            original_rel_set.add(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_relation_mapping  = {\n",
    "     'contains administrative territorial entity': 'contains',\n",
    "     'languages spoken, written or signed': 'language',\n",
    "     'language of work or name': 'language',\n",
    "     'located in the administrative territorial entity': 'located in',\n",
    "     'original language of film or TV show': 'original language',\n",
    "     'position played on team / speciality': 'position played',\n",
    "     'sports season of league or competition': 'sports season',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./cdr_rand_200.json', 'r') as f:\n",
    "    cdr = json.load(f)\n",
    "    \n",
    "import gzip\n",
    "\n",
    "id2name = json.load(open('../datasets/cdr/id2name.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cdr_processed = defaultdict(list)\n",
    "\n",
    "for text in cdr.keys():\n",
    "    triple_list = cdr[text]\n",
    "    for triple in triple_list:\n",
    "        relation =  triple[3]\n",
    "        if relation == '1:CID:2':\n",
    "            if triple[0] not in id2name or triple[1] not in id2name:\n",
    "                continue\n",
    "            term_1, term_2 = id2name[triple[0]], id2name[triple[1]]\n",
    "            new_triple = [term_2, 'induced by', term_1]\n",
    "            cdr_processed[text].append(new_triple)\n",
    "        elif relation == '1:NR:2':\n",
    "            if triple[0] not in id2name or triple[1] not in id2name:\n",
    "                continue\n",
    "            term_1, term_2 = id2name[triple[0]], id2name[triple[1]]\n",
    "            new_triple = [term_2, 'not induced by', term_1]\n",
    "            cdr_processed[text].append(new_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/cdr_processed.json', 'w') as file:\n",
    "    json.dump(cdr_processed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./docred_rand_200.json\", 'r') as file:\n",
    "    docred = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "docred_process = defaultdict(list)\n",
    "\n",
    "for text in docred.keys():\n",
    "    triples = docred[text]\n",
    "    for triple in triples:\n",
    "        relation = triple[2]\n",
    "        if relation in wiki_relation_mapping:\n",
    "            relation = wiki_relation_mapping[relation]\n",
    "        new_triple = [triple[0], relation, triple[1]]\n",
    "        docred_process[text].append(new_triple)\n",
    "        \n",
    "with open('./processed/docred_processed.json', 'w') as file:\n",
    "    json.dump(docred_process, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYT10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./nyt10m_rand_500.json\", 'r') as file:\n",
    "    nyt10m = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "nyt10m_processed = defaultdict(list)\n",
    "\n",
    "for text in nyt10m.keys():\n",
    "    triples = nyt10m[text]\n",
    "    for triple in triples:\n",
    "        relation = triple[2].split('/')[-1].replace('_', ' ')\n",
    "        new_triple = [triple[0], relation, triple[1]]\n",
    "        nyt10m_processed[text].append(new_triple)\n",
    "\n",
    "with open('./processed/nyt10m_processed.json', 'w') as file:\n",
    "    json.dump(nyt10m_processed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki20m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./wiki20m_rand_500.json\", 'r') as file:\n",
    "    wiki20m = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "wiki20m_processed = defaultdict(list)\n",
    "\n",
    "for text in wiki20m.keys():\n",
    "    triples = wiki20m[text]\n",
    "    for triple in triples:\n",
    "        term_1, term_2, relation = triple\n",
    "        # if [term_2, relation, term_1] in wiki20m_processed[text]:\n",
    "        #     continue\n",
    "        if relation in wiki_relation_mapping:\n",
    "            relation = wiki_relation_mapping[relation]\n",
    "        new_triple = [term_1, relation, term_2]\n",
    "        wiki20m_processed[text].append(new_triple)\n",
    "\n",
    "\n",
    "with open('./processed/wiki20m_processed.json', 'w') as file:\n",
    "    json.dump(wiki20m_processed, file, indent=4)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TACRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./tacred_rand_800.json\", 'r') as file:\n",
    "    tacred = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tacred_processed = defaultdict(list)\n",
    "\n",
    "for text in tacred.keys():\n",
    "    triples = tacred[text]\n",
    "    for triple in triples:\n",
    "        relation = triple[2]\n",
    "        if relation == 'NA':\n",
    "            continue\n",
    "        else:\n",
    "            relation = triple[2].split(':')[1].split('/')[-1]\n",
    "        new_triple = [triple[0], relation, triple[1]]\n",
    "        tacred_processed[text].append(new_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/tacred_processed.json', 'w') as file:\n",
    "    json.dump(tacred_processed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./wiki80_rand_800.json\", 'r') as file:\n",
    "    wiki80 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "wiki80_processed = defaultdict(list)\n",
    "\n",
    "for text in wiki80.keys():\n",
    "    triples = wiki80[text]\n",
    "    for triple in triples:\n",
    "        relation = triple[2]\n",
    "        if relation in wiki_relation_mapping:\n",
    "            relation = wiki_relation_mapping[relation]\n",
    "        new_triple = [triple[0], relation, triple[1]]\n",
    "        wiki80_processed[text].append(new_triple)\n",
    "        \n",
    "with open('./processed/wiki80_processed.json', 'w') as file:\n",
    "    json.dump(wiki80_processed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:06<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai_emb import embedding_retriever\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('/data/pj20/gre_element_embedding_dict.json', 'r') as f:\n",
    "    element_embedding_dict = json.load(f)\n",
    "    \n",
    "with open('./processed/cdr_processed.json', 'r') as file:\n",
    "    cdr = json.load(file)\n",
    "    \n",
    "for text in tqdm(cdr.keys()):\n",
    "    triples = cdr[text]\n",
    "    for triple in triples:\n",
    "        term_1, relation, term_2 = triple\n",
    "        if term_1 not in element_embedding_dict:\n",
    "            element_embedding_dict[term_1] = embedding_retriever(term_1)\n",
    "        if term_2 not in element_embedding_dict:\n",
    "            element_embedding_dict[term_2] = embedding_retriever(term_2)\n",
    "        if relation not in element_embedding_dict:\n",
    "            element_embedding_dict[relation] = embedding_retriever(relation)\n",
    "        \n",
    "with open('/data/pj20/gre_element_embedding_dict.json', 'w') as f:\n",
    "    json.dump(element_embedding_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:09<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../processed_results/cdr_rand_200_gpt-3.5_semi_1.json', 'r') as file:\n",
    "    cdr = json.load(file)\n",
    "    \n",
    "for text in tqdm(cdr.keys()):\n",
    "    triples = cdr[text]\n",
    "    for triple in triples:\n",
    "        term_1, relation, term_2 = triple\n",
    "        if term_1 not in element_embedding_dict:\n",
    "            element_embedding_dict[term_1] = embedding_retriever(term_1)\n",
    "        if term_2 not in element_embedding_dict:\n",
    "            element_embedding_dict[term_2] = embedding_retriever(term_2)\n",
    "        if relation not in element_embedding_dict:\n",
    "            element_embedding_dict[relation] = embedding_retriever(relation)\n",
    "        \n",
    "with open('/data/pj20/gre_element_embedding_dict.json', 'w') as f:\n",
    "    json.dump(element_embedding_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 209453.38it/s]\n",
      " 22%|██▏       | 108/500 [00:13<00:46,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"place_of_birth, nationality, location, place_lived\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 136/500 [00:20<00:40,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"leader of the Shiite Amal party and speaker of Lebanon's Parliament\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 379/500 [01:08<00:09, 12.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"administrative_divisions\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:38<00:00,  3.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 219597.07it/s]\n",
      " 38%|███▊      | 191/500 [00:19<00:45,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"Delawarean\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 270/500 [00:32<00:33,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"doubt\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [01:00<00:04, 31.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"position:administrative_division\". Retrying...\n",
      "Error in gpt_instruct: \"new money\". Retrying...\n",
      "Error in gpt_instruct: \"new money\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 427/500 [03:25<00:22,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in gpt_instruct: \"airport terminal:location\". Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai_emb import embedding_retriever\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "model_names = [\n",
    "    # 'vicuna-1.3-33b', \n",
    "    # 'llama-2-70b',\n",
    "    # 'gpt-3.5-turbo-1106',\n",
    "    # 'gpt-4-1106-preview',\n",
    "    # 'openchat',\n",
    "    'gpt-3.5_closed',\n",
    "    'gpt-3.5_semi'\n",
    "    ]\n",
    "\n",
    "dataset_names = [\n",
    "    'cdr_rand_200',\n",
    "    'nyt10m_rand_500',\n",
    "]\n",
    "\n",
    "seeds = [1]\n",
    "\n",
    "        \n",
    "# with open('/data/pj20/gre_element_embedding_dict.json', 'r') as f:\n",
    "#     element_embedding_dict = json.load(f)\n",
    "    \n",
    "for model_name in model_names:\n",
    "    for dataset_name in dataset_names:\n",
    "        for seed in seeds:\n",
    "            file_to_evaluate = f'../processed_results/{dataset_name}_{model_name}_{seed}.json'\n",
    "            text_triples = json.load(open(file_to_evaluate, 'r'))\n",
    "    \n",
    "            for text in tqdm(text_triples.keys()):\n",
    "                triples = text_triples[text]\n",
    "                for triple in triples:\n",
    "                    term_1, relation, term_2 = triple\n",
    "                    if term_1 not in element_embedding_dict:\n",
    "                        element_embedding_dict[term_1] = embedding_retriever(term_1)\n",
    "                    if term_2 not in element_embedding_dict:\n",
    "                        element_embedding_dict[term_2] = embedding_retriever(term_2)\n",
    "                    if relation not in element_embedding_dict:\n",
    "                        element_embedding_dict[relation] = embedding_retriever(relation)\n",
    "                    \n",
    "                    \n",
    "with open('/data/pj20/gre_element_embedding_dict.json', 'w') as f:\n",
    "    json.dump(element_embedding_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('textgen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72e4df05eb8a5f0bdb80d33a9292878c0e2ee2d3f5bd7213be092f7893c82c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
