{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"results\"\n",
    "metrics_names = ['Completeness', 'Factualness', 'Granularity', 'Topical', 'Uniqueness']\n",
    "\n",
    "results_files = glob.glob(os.path.join(cache_dir, \"*.csv\"))\n",
    "results_files = [os.path.basename(f) for f in results_files]\n",
    "\n",
    "# group them by metrics_name according to the start of the file name\n",
    "results_files_grouped = defaultdict(list)\n",
    "for f in results_files:\n",
    "    for m in metrics_names:\n",
    "        if f.startswith(m):\n",
    "            results_files_grouped[m].append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_group_1_dict = {}\n",
    "human_group_2_dict = {}\n",
    "\n",
    "for metric_name, files in results_files_grouped.items():\n",
    "    jc_data = []\n",
    "    zf_data = []\n",
    "    pt_data = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(cache_dir, file))\n",
    "        if 'Jiacheng' in file:\n",
    "            jc_data.append(df)\n",
    "        elif 'Zifeng' in file:\n",
    "            # extract sample_id comlumn (the first colmn) equals between 20 and 69\n",
    "            zf_data.append(df[df['sample_id'].between(20, 69)])\n",
    "        else:\n",
    "            pt_data.append(df[df['sample_id'].between(20, 69)])\n",
    "\n",
    "    human_group_1_dict[metric_name] = pd.concat(jc_data)\n",
    "    human_group_2_dict[metric_name] = pd.concat(pt_data + zf_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_group_1_dict['Topical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_group_2_dict['Topical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_agreement(human_1, human_2, metric_name):\n",
    "    agree = 0.\n",
    "    total = 0.\n",
    "    for i in range(len(human_1)):\n",
    "        if human_1.iloc[i].win == human_2.iloc[i].win:\n",
    "            agree += 1\n",
    "        elif human_1.iloc[i].win == \"tie\" or human_2.iloc[i].win == \"tie\":\n",
    "            agree += 0.5\n",
    "        total += 1\n",
    "\n",
    "    print(f'{metric_name}: human aggreement score is: {agree/total}')\n",
    "\n",
    "for metric_name in metrics_names:\n",
    "    cal_agreement(human_group_1_dict[metric_name], human_group_2_dict[metric_name], metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_single = {}\n",
    "for metric_name, files in results_files_grouped.items():\n",
    "    data_curr = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(cache_dir, file))\n",
    "        if 'Zifeng' in file:\n",
    "            data_curr.append(df[df['sample_id'].between(70, 99)])\n",
    "        elif 'Patrick' in file:\n",
    "            data_curr.append(df[df['sample_id'].between(0, 19)])\n",
    "    \n",
    "    data_merged_single[metric_name] = pd.concat(data_curr)\n",
    "\n",
    "data_merged_single['Topical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 20-69 data\n",
    "\n",
    "def merge_humans(human_1, human_2):\n",
    "    raw_data_new = pd.DataFrame({'sample_id': [], 'model_A_name': [], 'model_B_name': [], \"win\":[]}) \n",
    "    \n",
    "    for i in tqdm(range(len(human_1))):\n",
    "        if human_1.iloc[i].win == human_2.iloc[i].win:\n",
    "            raw_data_new.loc[len(raw_data_new)] = [human_1.iloc[i].sample_id, human_1.iloc[i].model_A_name, human_1.iloc[i].model_B_name, human_1.iloc[i].win]\n",
    "        elif human_1.iloc[i].win == \"tie\":\n",
    "            raw_data_new.loc[len(raw_data_new)] = [human_2.iloc[i].sample_id, human_2.iloc[i].model_A_name, human_2.iloc[i].model_B_name, human_2.iloc[i].win]\n",
    "        elif human_2.iloc[i].win == \"tie\":\n",
    "            raw_data_new.loc[len(raw_data_new)] = [human_1.iloc[i].sample_id, human_1.iloc[i].model_A_name, human_1.iloc[i].model_B_name, human_1.iloc[i].win]\n",
    "        else:\n",
    "            raw_data_new.loc[len(raw_data_new)] = [human_1.iloc[i].sample_id, human_1.iloc[i].model_A_name, human_1.iloc[i].model_B_name, \"tie\"]\n",
    "\n",
    "    return raw_data_new\n",
    "\n",
    "data_merged_twice = {}\n",
    "\n",
    "\n",
    "for metrics_name in metrics_names:\n",
    "    data_merged_twice[metrics_name] = merge_humans(human_group_1_dict[metrics_name], human_group_2_dict[metrics_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_twice['Topical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge single and twice\n",
    "data_merged = {}\n",
    "for metrics_name in metrics_names:\n",
    "    data_merged[metrics_name] = pd.concat([data_merged_single[metrics_name], data_merged_twice[metrics_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged['Completeness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elo(battles, K, SCALE, BASE, INIT_RATING):\n",
    "    rating = defaultdict(lambda: INIT_RATING)\n",
    "\n",
    "    for rd, model_a, model_b, win in battles[['model_A_name', 'model_B_name', 'win']].itertuples():\n",
    "        ra = rating[model_a]\n",
    "        rb = rating[model_b]\n",
    "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
    "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
    "        if win == \"model_A_win\":\n",
    "            sa = 1\n",
    "        elif win == \"model_B_win\":\n",
    "            sa = 0\n",
    "        elif win == \"tie\" or win == \"tie (bothbad)\":\n",
    "            sa = 0.5\n",
    "        else:\n",
    "            raise Exception(f\"unexpected vote {win}\")\n",
    "            \n",
    "        rating[model_a] += K * (sa - ea)\n",
    "        rating[model_b] += K * (1 - sa - eb)\n",
    "\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preety_print_elo_ratings(elo_ratings):\n",
    "    df = pd.DataFrame([\n",
    "        [n, elo_ratings[n]] for n in elo_ratings.keys()\n",
    "    ], columns=[\"Model\", \"Elo rating\"]).sort_values(\"Elo rating\", ascending=False).reset_index(drop=True)\n",
    "    df[\"Elo rating\"] = (df[\"Elo rating\"] + 0.5).astype(int)\n",
    "    df.index = df.index + 1\n",
    "    return df\n",
    "\n",
    "param_K=16\n",
    "param_SCALE=400\n",
    "param_BASE=10\n",
    "param_INIT_RATING=1000\n",
    "\n",
    "elo_ratings_dict = {}\n",
    "\n",
    "for metric_name in metrics_names:\n",
    "    elo_ratings = compute_elo(data_merged[metric_name], K=param_K, SCALE=param_SCALE, BASE=param_BASE, INIT_RATING=param_INIT_RATING)\n",
    "    print(metric_name)\n",
    "    print(preety_print_elo_ratings(elo_ratings))\n",
    "    elo_ratings_dict[metric_name] = elo_ratings\n",
    "    # save to txt\n",
    "    with open(f'output/{metric_name}_elo_ratings.txt', 'w') as f:\n",
    "        f.write(preety_print_elo_ratings(elo_ratings).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     \"Vicuna-7B\": 1222,\n",
    "#     \"ChatGLM\": 1172,\n",
    "#     \"Moss\": 1162,\n",
    "#     \"StableLM-Tuned-Alpha\": 1124,\n",
    "#     \"Alpaca-7B\": 1094,\n",
    "#     \"Open-Assistant\": 1087,\n",
    "#     \"Alpaca-lora\": 998,\n",
    "#     \"Dolly-v2-7B\": 972,\n",
    "#     \"MPT\": 960,\n",
    "#     \"Galatica\": 916,\n",
    "#     \"RWKV (Pile)-7B\": 903,\n",
    "#     \"BELLE\": 900,\n",
    "#     \"PandaLM\": 898,\n",
    "#     \"RedPajama-7B_base\": 889,\n",
    "#     \"h2oGPT-6.9B\": 873,\n",
    "#     \"RedPajama-7B_instruct\": 831\n",
    "# }\n",
    "\n",
    "def plot_elo_bar_plot(data, metric_name):\n",
    "\n",
    "    model_names = list(data.keys())\n",
    "    elo_ratings = list(data.values())\n",
    "\n",
    "    plot_data = pd.DataFrame({\"Model\": model_names, \"Elo Rating\": elo_ratings})\n",
    "\n",
    "    # colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A', '#19D3F3', '#FF6692', '#B6E880', '#FF97FF', '#FECB52', '#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A', '#19D3F3']\n",
    "\n",
    "    # # 创建图形\n",
    "    # fig = go.Figure()\n",
    "\n",
    "    # # 添加水平柱状图，并为每个柱子指定颜色\n",
    "    # for name, score, color in zip(model_names,elo_ratings, colors):\n",
    "    #     fig.add_trace(go.Bar(\n",
    "    #         x=[name],\n",
    "    #         y=[score],\n",
    "    #         marker=dict(color=color)\n",
    "    #     ))\n",
    "    font_size = 12\n",
    "\n",
    "    fig = px.bar(plot_data, x='Model', y='Elo Rating', color='Elo Rating', color_continuous_scale='Tealgrn')\n",
    "    fig.update_traces(texttemplate='%{y}', textposition='outside', textfont=dict(size=font_size))\n",
    "    fig.update_layout(xaxis_title=None, height=500, width=400, showlegend=False,\n",
    "                    yaxis=dict(\n",
    "                        categoryorder='total ascending', # 这会使项目按得分升序排列\n",
    "                        range=[min(elo_ratings) - 20, max(elo_ratings) + 80] # 可以通过调整这些值来限制y轴的显示范围\n",
    "                    ))\n",
    "    \n",
    "\n",
    "    # 获取当前的X轴刻度位置\n",
    "    tickvals = [i for i in range(len(plot_data['Model'].unique()))]\n",
    "\n",
    "    # 创建平移后的刻度位置（例如，向左平移0.2个单位）\n",
    "    new_tickvals = [tick - 0.2 for tick in tickvals]\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=new_tickvals,\n",
    "            ticktext=plot_data['Model'].unique(), # 使用原始的刻度标签\n",
    "            title_font=dict(size=16), # 调整X轴标题字体大小\n",
    "            tickfont=dict(size=font_size)    # 调整X轴刻度字体大小\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title_font=dict(size=16), # 调整Y轴标题字体大小\n",
    "            showticklabels=False,   # 调整Y轴刻度字体大小\n",
    "        ),\n",
    "        coloraxis_showscale=False # 隐藏侧边的颜色刻度表\n",
    "    )\n",
    "    fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=-1,\n",
    "            y0=1000,\n",
    "            x1=4,\n",
    "            y1=1000,\n",
    "            line=dict(\n",
    "                color=\"darkred\",\n",
    "            )\n",
    "    )\n",
    "\n",
    "    fig.write_image(f\"output/elo_ranking_{metric_name}.pdf\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# for each elements in elo_rating, int it   \n",
    "for metric_name in metrics_names:\n",
    "    elo_ratings_dict[metric_name] = {k: int(v) for k, v in elo_ratings_dict[metric_name].items()}\n",
    "\n",
    "for metric_name in metrics_names:\n",
    "    print(metric_name)\n",
    "    plot_elo_bar_plot(elo_ratings_dict[metric_name], metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
