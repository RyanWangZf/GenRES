{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"results/gre\"\n",
    "metrics_names = ['Completeness', 'Factualness', 'Granularity', 'Topical', 'Uniqueness']\n",
    "\n",
    "results_files = glob.glob(os.path.join(cache_dir, \"*.csv\"))\n",
    "results_files = [os.path.basename(f) for f in results_files]\n",
    "\n",
    "# group them by metrics_name according to the start of the file name\n",
    "results_files_grouped = defaultdict(list)\n",
    "for f in results_files:\n",
    "    for m in metrics_names:\n",
    "        if f.startswith(m):\n",
    "            results_files_grouped[m].append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Completeness': ['Completeness_GREScores.csv'],\n",
       "             'Factualness': ['Factualness_GREScores.csv'],\n",
       "             'Granularity': ['Granularity_GREScores.csv'],\n",
       "             'Topical': ['Topical_GREScores.csv'],\n",
       "             'Uniqueness': ['Uniqueness_GREScores.csv']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_files_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = {}\n",
    "for metric_name, files in results_files_grouped.items():\n",
    "    file = files[0]\n",
    "    df = pd.read_csv(os.path.join(cache_dir, file))\n",
    "    data_merged[metric_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elo(battles, K, SCALE, BASE, INIT_RATING):\n",
    "    rating = defaultdict(lambda: INIT_RATING)\n",
    "\n",
    "    for rd, model_a, model_b, win in battles[['model_A_name', 'model_B_name', 'win']].itertuples():\n",
    "        ra = rating[model_a]\n",
    "        rb = rating[model_b]\n",
    "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
    "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
    "        if win == \"model_A_win\":\n",
    "            sa = 1\n",
    "        elif win == \"model_B_win\":\n",
    "            sa = 0\n",
    "        elif win == \"tie\" or win == \"tie (bothbad)\":\n",
    "            sa = 0.5\n",
    "        else:\n",
    "            raise Exception(f\"unexpected vote {win}\")\n",
    "            \n",
    "        rating[model_a] += K * (sa - ea)\n",
    "        rating[model_b] += K * (1 - sa - eb)\n",
    "\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness\n",
      "         Model  Elo rating\n",
      "1  Groundtruth        1198\n",
      "2     OpenChat         963\n",
      "3  GPT-4-Turbo         949\n",
      "4   LLaMA2-70b         891\n",
      "Factualness\n",
      "         Model  Elo rating\n",
      "1     OpenChat        1123\n",
      "2   LLaMA2-70b        1081\n",
      "3  GPT-4-Turbo        1079\n",
      "4  Groundtruth         717\n",
      "Granularity\n",
      "         Model  Elo rating\n",
      "1   LLaMA2-70b        1021\n",
      "2  GPT-4-Turbo        1021\n",
      "3     OpenChat         995\n",
      "4  Groundtruth         962\n",
      "Topical\n",
      "         Model  Elo rating\n",
      "1     OpenChat        1219\n",
      "2  GPT-4-Turbo        1160\n",
      "3   LLaMA2-70b        1006\n",
      "4  Groundtruth         615\n",
      "Uniqueness\n",
      "         Model  Elo rating\n",
      "1     OpenChat        1061\n",
      "2   LLaMA2-70b        1037\n",
      "3  GPT-4-Turbo        1036\n",
      "4  Groundtruth         866\n"
     ]
    }
   ],
   "source": [
    "def preety_print_elo_ratings(elo_ratings):\n",
    "    df = pd.DataFrame([\n",
    "        [n, elo_ratings[n]] for n in elo_ratings.keys()\n",
    "    ], columns=[\"Model\", \"Elo rating\"]).sort_values(\"Elo rating\", ascending=False).reset_index(drop=True)\n",
    "    df[\"Elo rating\"] = (df[\"Elo rating\"] + 0.5).astype(int)\n",
    "    df.index = df.index + 1\n",
    "    return df\n",
    "\n",
    "param_K=16\n",
    "param_SCALE=400\n",
    "param_BASE=10\n",
    "param_INIT_RATING=1000\n",
    "\n",
    "elo_ratings_dict = {}\n",
    "\n",
    "for metric_name in metrics_names:\n",
    "    elo_ratings = compute_elo(data_merged[metric_name], K=param_K, SCALE=param_SCALE, BASE=param_BASE, INIT_RATING=param_INIT_RATING)\n",
    "    print(metric_name)\n",
    "    print(preety_print_elo_ratings(elo_ratings))\n",
    "    elo_ratings_dict[metric_name] = elo_ratings\n",
    "    # save to txt\n",
    "    with open(f'output/gre/{metric_name}_elo_ratings.txt', 'w') as f:\n",
    "        f.write(preety_print_elo_ratings(elo_ratings).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness\n",
      "Factualness\n",
      "Granularity\n",
      "Topical\n",
      "Uniqueness\n"
     ]
    }
   ],
   "source": [
    "data_merged_human = {}\n",
    "\n",
    "for metric_name in metrics_names:\n",
    "    file_dir = 'results/human'\n",
    "    file_list = glob.glob(os.path.join(file_dir, f\"{metric_name}_*.csv\"))\n",
    "    file = file_list[0]\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    data_merged_human[metric_name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>model_A_name</th>\n",
       "      <th>model_B_name</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Groundtruth</td>\n",
       "      <td>LLaMA2-70b</td>\n",
       "      <td>model_A_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Groundtruth</td>\n",
       "      <td>Openchat</td>\n",
       "      <td>model_A_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Groundtruth</td>\n",
       "      <td>GPT-4-Turbo</td>\n",
       "      <td>model_A_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>LLaMA2-70b</td>\n",
       "      <td>Openchat</td>\n",
       "      <td>model_A_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>LLaMA2-70b</td>\n",
       "      <td>GPT-4-Turbo</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>69</td>\n",
       "      <td>Groundtruth</td>\n",
       "      <td>Openchat</td>\n",
       "      <td>model_A_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>69</td>\n",
       "      <td>Groundtruth</td>\n",
       "      <td>GPT-4-Turbo</td>\n",
       "      <td>model_A_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>69</td>\n",
       "      <td>LLaMA2-70b</td>\n",
       "      <td>Openchat</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>69</td>\n",
       "      <td>LLaMA2-70b</td>\n",
       "      <td>GPT-4-Turbo</td>\n",
       "      <td>model_B_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>69</td>\n",
       "      <td>Openchat</td>\n",
       "      <td>GPT-4-Turbo</td>\n",
       "      <td>model_B_win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_id model_A_name model_B_name          win\n",
       "0            0  Groundtruth   LLaMA2-70b  model_A_win\n",
       "1            0  Groundtruth     Openchat  model_A_win\n",
       "2            0  Groundtruth  GPT-4-Turbo  model_A_win\n",
       "3            0   LLaMA2-70b     Openchat  model_A_win\n",
       "4            0   LLaMA2-70b  GPT-4-Turbo          tie\n",
       "..         ...          ...          ...          ...\n",
       "595         69  Groundtruth     Openchat  model_A_win\n",
       "596         69  Groundtruth  GPT-4-Turbo  model_A_win\n",
       "597         69   LLaMA2-70b     Openchat          tie\n",
       "598         69   LLaMA2-70b  GPT-4-Turbo  model_B_win\n",
       "599         69     Openchat  GPT-4-Turbo  model_B_win\n",
       "\n",
       "[600 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged_human['Completeness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness: human aggreement score is: 0.7958333333333333\n",
      "Factualness: human aggreement score is: 0.6225\n",
      "Granularity: human aggreement score is: 0.6516666666666666\n",
      "Topical: human aggreement score is: 0.7191666666666666\n",
      "Uniqueness: human aggreement score is: 0.6608333333333334\n"
     ]
    }
   ],
   "source": [
    "def cal_agreement(human_1, human_2, metric_name):\n",
    "    agree = 0.\n",
    "    total = 0.\n",
    "    for i in range(len(human_1)):\n",
    "        if human_1.iloc[i].win == human_2.iloc[i].win:\n",
    "            agree += 1\n",
    "        elif human_1.iloc[i].win == \"tie\" or human_2.iloc[i].win == \"tie\":\n",
    "            agree += 0.5\n",
    "        total += 1\n",
    "\n",
    "    print(f'{metric_name}: human aggreement score is: {agree/total}')\n",
    "\n",
    "for metric_name in metrics_names:\n",
    "    cal_agreement(data_merged_human[metric_name], data_merged[metric_name], metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
